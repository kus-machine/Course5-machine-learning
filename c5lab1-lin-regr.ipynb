{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\n#import seaborn as sns\nimport matplotlib.pyplot as plt\n#from matplotlib.colors import ListedColormap\n#from sklearn.model_selection import train_test_split\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-25T19:38:23.914338Z","iopub.execute_input":"2021-09-25T19:38:23.915634Z","iopub.status.idle":"2021-09-25T19:38:23.922143Z","shell.execute_reply.started":"2021-09-25T19:38:23.915550Z","shell.execute_reply":"2021-09-25T19:38:23.920937Z"},"trusted":true},"execution_count":1694,"outputs":[]},{"cell_type":"code","source":"#Gradient descent and co\ndef d_da(x, y, a, b):\n    result = np.zeros(len(x))\n    for i in range(len(x)-1):\n        result[i] = (- a * x[i] - b + y[i]) * x[i]\n    return -2 * result.mean()\ndef d_db(x, y, a, b):\n    result = np.zeros(len(x))\n    for i in range(len(x)-1):\n        result[i] = y[i] - a*x[i] - b\n    return -2 * result.mean()\ndef grad_desc(x, y, aa, bb, lambda_a, lambda_b, n_iter, a_tr, b_tr):\n    err = np.zeros(n_iter)\n    for n in range(n_iter):\n        aa = aa - lambda_a * d_da(x, y, aa, bb)\n        bb = bb - lambda_b * d_db(x, y, aa, bb)\n        err[n] = ((y-aa*x-bb)**2).mean()\n    return aa, bb, err\n","metadata":{"execution":{"iopub.status.busy":"2021-09-25T19:38:23.924297Z","iopub.execute_input":"2021-09-25T19:38:23.924628Z","iopub.status.idle":"2021-09-25T19:38:23.937535Z","shell.execute_reply.started":"2021-09-25T19:38:23.924569Z","shell.execute_reply":"2021-09-25T19:38:23.936236Z"},"trusted":true},"execution_count":1695,"outputs":[]},{"cell_type":"code","source":"#some params:\nnumber_of_points = 600\nn_iter = 500\na_true = 2 \nb_true = 10\nsigma_sq = 8 #degree of the Variance of Gaus's noize for our (ax+b) line\naa = 1 # start a for grad descent\nbb = 1 # start b for grad descent\nlambda_a = 0.0001 #coef of edu a\nlambda_b = 0.01 #coef of edu b\n\nx = np.linspace(0, int(number_of_points/10), number_of_points)\n#y(x) is a noized points near y=ax+b\ny = np.linspace(0, number_of_points/10, number_of_points) * a_true + b_true + np.random.normal(0, sigma_sq, (number_of_points))\n# plt.figure(figsize=(15,10))\n# plt.plot(x, y, 'b^')\n# plt.xlabel('X')\n# plt.ylabel('Y')\n# plt.grid()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T19:38:23.939659Z","iopub.execute_input":"2021-09-25T19:38:23.940115Z","iopub.status.idle":"2021-09-25T19:38:23.950154Z","shell.execute_reply.started":"2021-09-25T19:38:23.940068Z","shell.execute_reply":"2021-09-25T19:38:23.949114Z"},"trusted":true},"execution_count":1696,"outputs":[]},{"cell_type":"code","source":"#our grad desc\na_new, b_new, errors = grad_desc(x, y, aa, bb, lambda_a, lambda_b, n_iter, a_true, b_true)\n#solution from numpy\na_fit, b_fit = np.linalg.lstsq(np.vstack([x, np.ones(len(x))]).T, y, rcond=None)[0]\nprint(\"True a and b: \", (a_true, b_true))\nprint(\"Gradiend descent solution: \", (a_new, b_new))\nprint(\"Numpy linalg least square solution: \", (a_fit, b_fit))\nplt.figure(figsize=(15,11))\nplt.plot(x, y, 'b^', label = \"train data\")\nplt.plot(x, a_true*x + b_true, 'g', label='True line')\nplt.plot(x, a_fit*x + b_fit, 'b', label='Numpy linalg least square solution')\nplt.plot(x, a_new*x + b_new, 'r', label='Gradient descent solution')\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T19:38:23.952235Z","iopub.execute_input":"2021-09-25T19:38:23.952697Z","iopub.status.idle":"2021-09-25T19:38:25.300270Z","shell.execute_reply.started":"2021-09-25T19:38:23.952652Z","shell.execute_reply":"2021-09-25T19:38:25.299108Z"},"trusted":true},"execution_count":1697,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(n_iter), errors, 'r', label='Gradient descent error ')\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T19:38:25.301699Z","iopub.execute_input":"2021-09-25T19:38:25.301956Z","iopub.status.idle":"2021-09-25T19:38:25.505700Z","shell.execute_reply.started":"2021-09-25T19:38:25.301928Z","shell.execute_reply":"2021-09-25T19:38:25.504700Z"},"trusted":true},"execution_count":1698,"outputs":[]},{"cell_type":"code","source":"#sigmoid func\ndef sigm(z):\n    return 1/(1+np.exp(-z))\n#p^ from prezentation\ndef p(x, y, a, b):\n    f = sigm(x.dot(a) + b)\n    da = np.dot(x.T, (f - y))/x.shape[0]\n    db = (f - y).mean()\n    error = -(y * np.log(f) + (1-y) * np.log(1-f)).mean()\n    return da, db, error\n#grad descent func for 2d\ndef grad_2(x, y, n_iter, a0=1, b0=1, lmbd_a = 0.0001, lmbd_b = 0.01):\n    a = a0\n    b = b0\n    errors = np.zeros(n_iter)\n    for i in range(n_iter):\n        da, db, errors[i] = p(x, y, a, b)\n        a = a - lmbd_a * da\n        b = b - lmbd_b * db\n        \n    return a, b, errors\n#prediction func\ndef pred(x, a, b):\n    pred = np.zeros(x.shape[0])\n    f = sigm(x.dot(a) + b)\n    for i in range(x.shape[0]):\n        if(f[i] >= 0.5):\n            pred[i] = 1\n    return pred","metadata":{"execution":{"iopub.status.busy":"2021-09-25T19:38:25.507033Z","iopub.execute_input":"2021-09-25T19:38:25.507282Z","iopub.status.idle":"2021-09-25T19:38:25.524212Z","shell.execute_reply.started":"2021-09-25T19:38:25.507255Z","shell.execute_reply":"2021-09-25T19:38:25.523471Z"},"trusted":true},"execution_count":1699,"outputs":[]},{"cell_type":"code","source":"x_1 = np.random.multivariate_normal([1,5], [[2, 0], [0, 2]], (number_of_points))#, np.zeros(number_of_points, int)])\nx_2 = np.random.multivariate_normal([4,2], [[1, 0], [0, 1]], (number_of_points))#, np.ones(number_of_points, int)])\ny_1 = np.zeros(number_of_points, int)\ny_2 = np.ones(number_of_points, int)\nplt.figure(figsize=(15,11))\nplt.plot(x_1[...,0], x_1[...,1], 'b.', label = \"class 0\")\nplt.plot(x_2[...,0], x_2[...,1], 'y.', label = \"class 1\")\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T19:38:25.525245Z","iopub.execute_input":"2021-09-25T19:38:25.525484Z","iopub.status.idle":"2021-09-25T19:38:25.796577Z","shell.execute_reply.started":"2021-09-25T19:38:25.525458Z","shell.execute_reply":"2021-09-25T19:38:25.795543Z"},"trusted":true},"execution_count":1700,"outputs":[]},{"cell_type":"code","source":"n_iter2 = 3000\nstart_a = np.array([0,0])\nstart_b = 0\nlmbd_a = 0.01\nlmbd_b = 0.01\nx = np.concatenate([x_1,x_2])\ny = np.hstack([y_1,y_2])\n\na, b, errors = grad_2(x, y, n_iter2, start_a, start_b, lmbd_a, lmbd_b)\nhit = pred(x,a,b)\nprint(\"accuracy: \", 1 - abs(y-hit).mean())\n\nx_for_plot = np.linspace(x.min(), x.max(), x.shape[0])\nplt.figure(figsize=(15,11))\nplt.plot(x[...,0][hit==0], x[...,1][hit==0], 'r.', label = \"Predict class 0\")\nplt.plot(x[...,0][hit==1], x[...,1][hit==1], 'g.', label = \"Predict class 1\")\n#plt.plot(x_for_plot, x_for_plot * a[0] + b, 'r', label = \"line\")\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T19:38:25.798918Z","iopub.execute_input":"2021-09-25T19:38:25.799199Z","iopub.status.idle":"2021-09-25T19:38:26.565166Z","shell.execute_reply.started":"2021-09-25T19:38:25.799169Z","shell.execute_reply":"2021-09-25T19:38:26.564129Z"},"trusted":true},"execution_count":1701,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(n_iter2), errors, 'r', label='Error line')\nplt.grid()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T19:38:26.566569Z","iopub.execute_input":"2021-09-25T19:38:26.566854Z","iopub.status.idle":"2021-09-25T19:38:26.790930Z","shell.execute_reply.started":"2021-09-25T19:38:26.566826Z","shell.execute_reply":"2021-09-25T19:38:26.789764Z"},"trusted":true},"execution_count":1702,"outputs":[]}]}